\chapter{Background}\label{chap:background}
In solving sequence-to-sequence problems, such as machine translation (MT), where a ML algorithm translates speech or text to different languages, Recurrent Neural Networks (RNN) have been widely utilized in an encoder-decoder architecture, surpassing many other ML algorithms in performance~\cite{cho2014properties}. The encoder network takes the input data and generates a compact vector representation, which is then used as input for the decoder network. The decoder network unrolls over time to generate the output sequence based on the input and its internal state.\\
However, RNN face challenges with long sequences due to the vanishing gradient problem, which results in the model losing meaningful information about token at the start of a sequence~\cite{hochreiter1998vanishing}. To address this issue, improvements in architecture have been introduced, such as Long Short-Term Memory (LSTM)~\cite{hochreiter1997long} and Gated Recurrent Unit (GRU)~\cite{cho2014learning} models, which aim to preserve information from the entire sequence.\par
In recent years, the Transformer architecture, introduced by Vaswani et al.~\cite{vaswani2017attention}, has gained prominence. Similar to the RNN encoder-decoder architecture, it consists of an encoding and decoding component. The encoding component is a stack of encoder layers, and the decoding component is also a stack of decoder layers, both equal in number.\\
Each layer in the Transformer architecture consists of two sub-layers: a self-attention layer and a feed-forward neural network. The key difference between RNN and the Transformer lies in the self-attention sub-layer. This unique component enables the Transformer to consider all hidden states from the encoder sequence when making predictions. In the RNN encoder-decoder architecture the decoder is presented only with the final output of the encoder layer. The attention based architecture, on the other hand, allows a model to attend every hidden state from each encoder node at every time step, which allows Transformer models to effectively handle long sequences.\par
\textit{BERT} (\textbf{B}idirectional \textbf{E}ncoder \textbf{R}epresentations from \textbf{T}ransformers), introduced by Devlin et al.~\cite{devlin2019bert}, is a language model designed to learn general language representations. At its core, it is a multi-layer bidirectional Transformer encoder stack that is pre-trained on a large corpus.\\
BERT uses WordPiece embeddings~\cite{wu2016google} with a vocabulary size of 30,000 tokens. Unknown words not represented in the vocabulary are embedded as a special unknown token ($[UNK]$). The first token in each input sequence is denoted as a special classification token ($[CLS]$). Sentence pairs are processed as one input sequence separated by a special character ($[SEP]$), and a learned embedding is assigned to each token to represent the sentence it belongs to.\\
The final input embedding consists of the summarized token embeddings, segment embeddings, and position embeddings of each token in the sequence. The authors present two architecture configurations that differ in the number of Transformer blocks, hidden size, and number of self-attention heads.
For the pre-training phase, they propose two unsupervised objectives:\\
The first is Masked Language Modeling (MLM), where a portion of the token in the input sequence are masked and replaced with a special mask token ($[MASK]$). During the pre-training the model predicts the masked token to learn a deep bidirectional representation. To avoid a mismatch between the training and fine-tuning process, token chosen to be masked are only masked 80\% of the time. In 10\% of the time, token are replaced by a random token and in the remaining 10\%, token are left as they are.\\
The second is Next Sentence Prediction (NSP), which allows the model to learn sentence relationships. Two sentences, A and B, are chosen from the training corpus and the model has to predict if sentence B follows sentence A. In 50\% of theses cases, sentence B succeeds sentence A. In the remaining cases sentence B is chosen randomly from the corpus.\\
The self-attention mechanism allows fine-tuning BERT for various different NLP tasks, which resulted in many broken records for language-based tasks~\cite{sun2019fine,nogueira2019passage,zampieri2019semeval}.\par
Liu et al. introduced \textit{RoBERTa} (\textbf{R}obustly \textbf{o}ptimized \textbf{BERT} \textbf{a}pproach)~\cite{liu2019roberta}, which presents an optimized version of BERT with a series of improvements. Instead of using fixed masking patterns for sequences, they propose dynamic masking, where the masking pattern is generated each time a sequence is fed to the model. They also remove the Next Sentence Prediction (NSP) objective and utilize full sentences sampled contiguously from one or more documents. In addition, they employ larger mini-batches to improve optimization speed and end-task performance. Lastly, RoBERTa utilizes a larger vocabulary with Byte-Pair Encoding (BPE)~\cite{sennrich2016neural}, which is a hybrid approach combining character- and word-level representations of token. This embedding relies on subword token, gathered through statistical analysis of the training corpus.\par
Conneau et al. introduced \textit{XLM} (Cross-Lingual \textbf{L}anguage \textbf{M}odel)~\cite{conneau2019cross} with the goal of extending the pre-training approach to multiple languages to enable cross-lingual understanding (XLU). They adopted the BPE algorithm to generate a shared vocabulary of subwords for all languages. For pre-training XLM, they employed several objectives.\\
One of them is Causal Language Modeling (CLM), where the objective is to predict the next word given the previous words in a monolingual sentence.\\
They also utilized the MLM objective on monolingual input sentences, similar to BERT, where random tokens are masked for the model to predict. Instead of using sentence pairs as in BERT, they used text snippets with a random number of sentences. Additionally, the authors used weights to balance the disproportional frequencies of tokens.\\
The Translation Language Modeling (TLM) objective is a supervised task where parallel sentences in different languages are masked. The model must predict the masked token by attending to the surrounding words in the same language or in the translation sentence.\\
Similar to BERT, the input embedding for XLM includes token embeddings, position embeddings, and language embeddings.\\
At the time of its introduction, XLM outperformed several state-of-the-art models in machine translation benchmarks.\par
\textit{XLM-RoBERTa} (XLM-R) is a Transformer-based multilingual masked language model introduced by Conneau et al.~\cite{conneau2019unsupervised}, which draws inspiration from the XLM approach. The authors formulate the \enquote{curse of multilinguality}, which refers to a tradeoff point where including more languages can improve cross-lingual performance for low-resource languages. However, beyond that threshold, the overall performance on both monolingual and cross-lingual benchmarks starts to worsen.\\
XLM-R utilizes a Transformer encoder model, trained solely with the MLM objective, similar to the XLM approach. For tokenization, they use subwords, but instead of BPE, they employ SentencePiece~\cite{kudo2018sentencepiece} with a unigram language model~\cite{kudo2018subword} and a large vocabulary of 250k subwords. The authors also remove the language embeddings from the input embeddings, which were introduced in XLM, to facilitate improved code-switching.\\
XLM-R is trained on 100 languages using a large CommonCrawl Corpus with a size of 2.5 TB. The authors provide two model configurations to enable comparisons with BERT. In their evaluation, they compare their results against XLM and mBERT, a BERT Transformer pre-trained on a multilingual corpus. XLM-RoBERTa achieves state-of-the-art performance on multiple cross-lingual tasks, including cross-lingual classification, sequence labeling, and question answering.\par
\textit{LayoutLM}, proposed by Xu et al.~\cite{xu2020layoutlm}, is a novel Transformer model designed to incorporate text, layout, and visual information from documents. The authors aim to leverage two additional features alongside the input text: Document Layout Information, which captures the relative position of each token in the document, and Visual Information, which captures essential features in Visual Rich Documents (VRD).\\
To integrate both features, the authors introduce two modifications to the BERT architecture: a 2-D position embedding and an image embedding.\\
The 2-D position embedding encodes the relative position of a token as a bounding box $(x_0, y_0, x_1, y_1)$, where $(x_0, y_0)$ is the location of the upper left corner, and $(x_1, y_1)$ represents the position of the lower right corner. Each coordinate dimension shares the same embedding table, resulting in four position embedding layers, each corresponding to one coordinate in the bounding box.\\
For the image embedding, the document images are divided into pieces based on the bounding boxes of each token. Image region features are then generated using a Faster R-CNN model~\cite{girshick2015fast} for these image parts. Additionally, a Region of Interest (ROI) feature map of the entire scanned document is generated using Faster R-CNN, specifically for the $[CLS]$ token. The image embedding layer, composed of the ROI feature map and region features, is added following the Transformer block and after the pre-training of the model.\\
In the pre-training phase, the authors introduce two learning objectives:\\
One is the Masked Visual-Language Model (MVLM) task, where input tokens are randomly masked while retaining their position embeddings. The model must predict the masked token based on its textual and visual context.\\
The second is the Multi-label Document Classification (MDC) task, where the model predicts one or more classes to which a document belongs.\\
The model is pre-trained using the IIT-CDIP Test Collection~\cite{lewis2006building} and achieved state-of-the-art results in various benchmarks related to document understanding.\newpage
\textit{LayoutLMv2}, a refined version of the LayoutLM architecture presented by Xu et al.~\cite{xu2020layoutlmv2}, introduces significant improvements by incorporating visual information during pre-training, a feature that its predecessor lacked. The authors propose several modifications for the embedding layers.\\
The Text Embedding layer follows the BERT approach, incorporating token, position, and segment embeddings using WordPiece for tokenization.\\
The Visual Embedding layer utilizes feature maps from the visual encoder, a ResNeXt-FPN model~\cite{xie2017aggregated}, enabling weight updates during backpropagation. These feature maps are average-pooled and flattened, resulting in a sequence of visual embeddings. A linear projection layer reduces the dimensionality of the visual embeddings to match that of the text embeddings. Additionally, the reduced visual embeddings are complemented with a  1-D position embedding and a separate segment embedding ($[C]$).\\
The Layout Embedding layer concatenates six bounding box features ($x_0, y_0,\allowbreak x_1, y_1,\allowbreak width, height$) to generate a token-level 2-D positional embedding. Additionally, an empty 2-D position token ($box_{PAD} = (0,0,0,0,0,0)$) is added for all special token ($[CLS]$, $[SEP]$, and $[PAD]$).\\
The final inputs for the Transformer block consists of the Visual/Text Token Embedding layer, 2-D Position Embedding layer, 1-D Position Embedding layer, and Segment Embedding layer.\\
To allow the model to learn relationships between input token and their relative position embeddings, the authors introduce a spatial-aware self-attention mechanism.\\
As pre-training objectives, they propose the MVLM, as introduced in LayoutLM, to predict randomly masked text token, while keeping layout information unchanged. Furthermore, image regions corresponding to the masked token are also masked, to prevent visual clue leakage.\\
Additionally, they propose the Text-Image Alignment (TIA) task, where token lines are arbitrary selected and the corresponding image regions are covered on the document. For each text token the model has to predict, if the token is covered or not.\\
In the last objective, Text-Image Matching (TIM), the model has to predict, if an image and text originate from the same document page.\\
The authors also employ two transformer configurations, comparable to BERT, and pre-train the model on the IIT-CDIP Test Collection, as utilized in LayoutLM.\\ LayoutLMv2 outperforms LayoutLM by a large margin in several downstream document understanding tasks.\newpage
\textit{LayoutXLM}, introduced by Xu et al.~\cite{xu2021layoutxlm}, utilizes the same architecture as LayoutLMv2 for multilingual document understanding. To account for different languages, the authors propose a slightly altered pre-training objective.\\
They propose the Multilingual Masked Visual-Language Modeling objective (MMVLM), where the task is the same as in the MVLM objective, to predict the masked text token based on its text and layout context, but obtain character-level instead of word-level bounding boxes, to prevent a language specific bias. The text is tokenized using SentencePiece with a unigram language model. The final bounding box of each token is calculated by merging its resulting character bounding boxes.
The TIA and TIM objectives are analogous to the LayoutLMv2 pre-training.\\
The pre-training corpus consists of 30 million scanned and digital-born documents in 53 languages.\\
The LayoutXLM model outperformed the state-of-the-art models for multilingual document understanding in several benchmarks.
